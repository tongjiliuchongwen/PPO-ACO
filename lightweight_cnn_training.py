# lightweight_cnn_training.py - è½»é‡ç‰ˆCNNè®­ç»ƒï¼ˆåŸºäºå¿«é€Ÿæµ‹è¯•æˆåŠŸé…ç½®ï¼‰

import os
import time
import numpy as np
import torch
import torch.optim as optim
import matplotlib.pyplot as plt

import config_matrix as config
from environment_cnn_pheromone import CNNPheromoneEnv
from aco_system_matrix import ACOSystemMatrix


class LightweightCNNNetwork(torch.nn.Module):
    """è½»é‡ç‰ˆCNN-LSTMç½‘ç»œï¼šåŸºäºå¿«é€Ÿæµ‹è¯•çš„æˆåŠŸé…ç½®"""

    def __init__(self, input_dim=12, action_dim=1, lstm_hidden_size=64, max_action=1.0):
        super(LightweightCNNNetwork, self).__init__()
        self.input_dim = input_dim
        self.action_dim = action_dim
        self.lstm_hidden_size = lstm_hidden_size
        self.max_action = max_action

        print(f"ğŸª¶ è½»é‡ç‰ˆCNN-LSTMç½‘ç»œ:")
        print(f"   è¾“å…¥ç»´åº¦: {input_dim}")
        print(f"   LSTMéšè—ç»´åº¦: {lstm_hidden_size} (ç®€åŒ–)")
        print(f"   åŠ¨ä½œç»´åº¦: {action_dim}")

        # ç®€åŒ–çš„è¾“å…¥é¢„å¤„ç†å±‚
        self.input_processor = torch.nn.Sequential(
            torch.nn.Linear(input_dim, 32),
            torch.nn.ReLU(),
            torch.nn.Linear(32, 16),
            torch.nn.ReLU()
        )

        # æ›´å°çš„LSTM
        self.lstm = torch.nn.LSTM(
            input_size=16,
            hidden_size=lstm_hidden_size,
            num_layers=1,
            batch_first=True
        )

        # ç®€åŒ–çš„è¾“å‡ºå¤´
        self.actor_head = torch.nn.Sequential(
            torch.nn.Linear(lstm_hidden_size, 32),
            torch.nn.ReLU(),
            torch.nn.Linear(32, action_dim)
        )

        self.critic_head = torch.nn.Sequential(
            torch.nn.Linear(lstm_hidden_size, 32),
            torch.nn.ReLU(),
            torch.nn.Linear(32, 1)
        )

        # ä¿å®ˆçš„å¯¹æ•°æ ‡å‡†å·®
        self.log_std = torch.nn.Parameter(torch.full((action_dim,), -1.0))

        # è½»é‡çº§æƒé‡åˆå§‹åŒ–
        self._init_weights()

    def _init_weights(self):
        """è½»é‡çº§æƒé‡åˆå§‹åŒ–"""
        for module in [self.input_processor, self.actor_head, self.critic_head]:
            for layer in module:
                if isinstance(layer, torch.nn.Linear):
                    torch.nn.init.orthogonal_(layer.weight, gain=0.5)
                    torch.nn.init.constant_(layer.bias, 0)

        # LSTMåˆå§‹åŒ–
        for name, param in self.lstm.named_parameters():
            if 'weight_ih' in name:
                torch.nn.init.xavier_uniform_(param.data)
            elif 'weight_hh' in name:
                torch.nn.init.orthogonal_(param.data, gain=0.5)
            elif 'bias' in name:
                torch.nn.init.constant_(param.data, 0)

    def forward(self, obs, hidden_state=None):
        if obs.dim() == 2:
            obs = obs.unsqueeze(1)
            single_step = True
        else:
            single_step = False

        batch_size, seq_len, _ = obs.shape

        # è¾“å…¥é¢„å¤„ç†
        processed_input = self.input_processor(obs)

        # LSTMå¤„ç†
        if hidden_state is None:
            h_0 = torch.zeros(1, batch_size, self.lstm_hidden_size, device=obs.device)
            c_0 = torch.zeros(1, batch_size, self.lstm_hidden_size, device=obs.device)
            hidden_state = (h_0, c_0)

        lstm_out, new_hidden_state = self.lstm(processed_input, hidden_state)

        # è¾“å‡ºå¤´
        action_mean = self.actor_head(lstm_out)
        value = self.critic_head(lstm_out)

        # å¤„ç†è¾“å‡ºç»´åº¦
        if single_step:
            action_mean = action_mean.squeeze(1)
            value = value.squeeze(1).squeeze(1)
        else:
            value = value.squeeze(-1)

        # ä¿å®ˆçš„log_stdé™åˆ¶
        log_std = torch.clamp(self.log_std, -2.0, 0.5)

        return action_mean, log_std, value, new_hidden_state

    def get_action_and_value(self, obs, hidden_state=None, deterministic=False):
        action_mean, log_std, value, new_hidden_state = self.forward(obs, hidden_state)

        if deterministic:
            action = torch.tanh(action_mean) * self.max_action
            log_prob = None
        else:
            std = torch.exp(log_std)
            dist = torch.distributions.Normal(action_mean, std)
            raw_action = dist.sample()

            action = torch.tanh(raw_action) * self.max_action

            log_prob = dist.log_prob(raw_action).sum(dim=-1)
            log_prob -= torch.sum(torch.log(self.max_action * (1 - torch.tanh(raw_action).pow(2)) + 1e-6), dim=-1)

        return action, log_prob, value, new_hidden_state

    def evaluate_actions(self, obs, actions, hidden_states=None):
        action_mean, log_std, values, _ = self.forward(obs, hidden_states)

        # åå‘è®¡ç®—raw_actions
        normalized_actions = actions / self.max_action
        normalized_actions = torch.clamp(normalized_actions, -1 + 1e-6, 1 - 1e-6)
        raw_actions = 0.5 * torch.log((1 + normalized_actions) / (1 - normalized_actions))

        # è®¡ç®—log_prob
        std = torch.exp(log_std)
        dist = torch.distributions.Normal(action_mean, std)
        log_probs = dist.log_prob(raw_actions).sum(dim=-1)

        # Tanhæ ¡æ­£
        log_probs -= torch.sum(torch.log(self.max_action * (1 - normalized_actions.pow(2)) + 1e-6), dim=-1)

        # ç†µ
        entropy = dist.entropy().sum(dim=-1)

        return log_probs, values, entropy


class LightweightCNNAgent:
    """è½»é‡ç‰ˆCNNæ™ºèƒ½ä½“"""

    def __init__(self, env, device='cpu'):
        self.env = env
        self.device = device
        self.obs_dim = env.observation_space.shape[0]
        self.action_dim = env.action_space.shape[0]

        # è·å–CNNç‰¹å¾æå–å™¨
        pheromone_extractor = env.get_pheromone_extractor()

        # è½»é‡ç‰ˆç½‘ç»œ
        self.policy_network = LightweightCNNNetwork(
            input_dim=self.obs_dim,
            action_dim=self.action_dim,
            lstm_hidden_size=64,  # å‡å°‘åˆ°64
            max_action=config.OMEGA_MAX
        ).to(self.device)

        # ä¿å®ˆçš„ä¼˜åŒ–å™¨è®¾ç½®
        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=5e-5)  # ä¿å®ˆå­¦ä¹ ç‡

        # ç®€åŒ–çš„PPOè¶…å‚æ•°
        self.gamma = 0.99
        self.gae_lambda = 0.95
        self.clip_ratio = 0.1  # æ›´ä¿å®ˆçš„è£å‰ª
        self.entropy_coef = 0.01
        self.value_coef = 0.5
        self.max_grad_norm = 0.3  # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
        self.n_updates_per_iteration = 3  # å‡å°‘æ›´æ–°æ¬¡æ•°
        self.timesteps_per_batch = 2000  # å‡å°‘æ‰¹æ¬¡å¤§å°

        # è½»é‡ç‰ˆACOå¼•å¯¼
        self.alpha_pref = 0.9
        self.beta_aco = 0.02  # æ›´è½»å¾®çš„å¼•å¯¼

        # ç»Ÿè®¡è®°å½•
        self.logger = {'batch_rews': [], 'batch_success_rate': []}
        self._lifetime_successes = 0
        self._global_step = 0

        # æ—©åœç›‘æ§
        self.success_history = []
        self.best_success_rate = 0.0
        self.patience_counter = 0
        self.patience_limit = 20  # 20æ¬¡è¿­ä»£æ— æ”¹å–„åˆ™æ—©åœ

    def get_action(self, observation, hidden_state=None, aco_system=None):
        """è½»é‡ç‰ˆåŠ¨ä½œé€‰æ‹©"""
        with torch.no_grad():
            obs_tensor = torch.from_numpy(observation).unsqueeze(0).to(self.device)
            action, log_prob, value, new_hidden = self.policy_network.get_action_and_value(
                obs_tensor, hidden_state
            )
            return action.cpu().numpy()[0], log_prob.cpu().item(), new_hidden

    def rollout(self, aco_system, episodes_per_batch=30):
        """è½»é‡ç‰ˆrolloutï¼šæ›´å°‘çš„episodesï¼Œæ›´å¿«çš„åé¦ˆ"""
        batch_obs = []
        batch_acts = []
        batch_logp = []
        batch_rews_list = []
        batch_lens = []
        batch_infos = []

        episode_count = 0
        with torch.no_grad():
            while episode_count < episodes_per_batch:
                try:
                    obs, _ = self.env.reset(major_reset=True)
                    hidden_state = None
                    ep_rews = []
                    ep_traj = []
                    done = truncated = False

                    while not (done or truncated):
                        self._global_step += 1
                        ep_traj.append(self.env.get_agent_position().copy())

                        action, log_prob, hidden_state = self.get_action(obs, hidden_state, aco_system)
                        next_obs, reward, done, truncated, info = self.env.step(action, aco_system)

                        batch_obs.append(obs)
                        batch_acts.append(action)
                        batch_logp.append(log_prob)
                        ep_rews.append(reward)

                        obs = next_obs

                    ep_traj.append(self.env.get_agent_position().copy())
                    batch_rews_list.append(ep_rews)
                    batch_lens.append(len(ep_rews))
                    batch_infos.append(info)

                    # æˆåŠŸå¤„ç†ï¼šæ›´æ¸©å’Œçš„ä¿¡æ¯ç´ æ²‰ç§¯
                    if info.get('target_found', False):
                        self._lifetime_successes += 1
                        path_quality = 1.5 / max(1, len(ep_traj))  # é€‚ä¸­çš„æ²‰ç§¯å¼ºåº¦
                        aco_system.deposit_navigation(ep_traj, path_quality)
                        print(f"ğŸ¯ è½»é‡ç‰ˆæˆåŠŸ! æ­¥æ•°: {len(ep_traj)}, æ€»æˆåŠŸ: {self._lifetime_successes}")

                    # æ¸©å’Œçš„è’¸å‘
                    aco_system.nav_evaporation_rate = 0.03
                    aco_system.evaporate()

                    episode_count += 1

                except Exception as e:
                    print(f"âš ï¸ Episodeå¤±è´¥: {e}")
                    episode_count += 1
                    continue

        # ç»Ÿè®¡
        self.logger['batch_rews'] = [sum(rs) for rs in batch_rews_list]
        num_eps = len(batch_lens)
        succ = sum(1 for inf in batch_infos if inf.get('target_found', False))
        self.logger['batch_success_rate'] = (succ / num_eps) if num_eps > 0 else 0.0

        return {
            'batch_obs': np.array(batch_obs, dtype=np.float32),
            'batch_acts': np.array(batch_acts, dtype=np.float32),
            'batch_log_probs': np.array(batch_logp, dtype=np.float32),
            'episode_lengths': batch_lens,
            'episode_rewards': batch_rews_list
        }

    def compute_advantages(self, rollout_data):
        """ç®€åŒ–çš„ä¼˜åŠ¿è®¡ç®—"""
        batch_obs = rollout_data['batch_obs']
        episode_lengths = rollout_data['episode_lengths']
        episode_rewards = rollout_data['episode_rewards']

        if len(batch_obs) == 0:
            return np.array([]), np.array([])

        with torch.no_grad():
            obs_tensor = torch.from_numpy(batch_obs).to(self.device)
            _, _, values, _ = self.policy_network.forward(obs_tensor, None)
            values = values.cpu().numpy()

        all_advantages = []
        all_returns = []

        start_idx = 0
        for i, ep_len in enumerate(episode_lengths):
            if i >= len(episode_rewards) or ep_len == 0:
                continue

            ep_values = values[start_idx:start_idx + ep_len]
            ep_rewards = episode_rewards[i]

            if len(ep_rewards) != ep_len:
                start_idx += ep_len
                continue

            # GAEè®¡ç®—
            advantages = np.zeros_like(ep_rewards, dtype=np.float32)
            last_advantage = 0.0

            for t in reversed(range(ep_len)):
                next_value = ep_values[t + 1] if t < ep_len - 1 else 0.0
                delta = ep_rewards[t] + self.gamma * next_value - ep_values[t]
                advantages[t] = delta + self.gamma * self.gae_lambda * last_advantage
                last_advantage = advantages[t]

            returns = advantages + ep_values
            all_advantages.extend(advantages)
            all_returns.extend(returns)
            start_idx += ep_len

        if len(all_advantages) == 0:
            return np.array([]), np.array([])

        all_advantages = np.array(all_advantages, dtype=np.float32)
        if all_advantages.std() > 1e-8:
            all_advantages = (all_advantages - all_advantages.mean()) / (all_advantages.std() + 1e-8)

        return all_advantages, np.array(all_returns, dtype=np.float32)

    def update_networks(self, rollout_data, advantages, returns):
        """è½»é‡ç‰ˆç½‘ç»œæ›´æ–°"""
        batch_obs = rollout_data['batch_obs']
        batch_acts = rollout_data['batch_acts']
        batch_log_probs = rollout_data['batch_log_probs']

        if len(batch_obs) == 0 or len(advantages) == 0:
            return

        obs_tensor = torch.from_numpy(batch_obs).float().to(self.device)
        acts_tensor = torch.from_numpy(batch_acts).float().to(self.device)
        old_log_probs = torch.from_numpy(batch_log_probs).float().to(self.device)
        advantages_tensor = torch.from_numpy(advantages).float().to(self.device)
        returns_tensor = torch.from_numpy(returns).float().to(self.device)

        # æ›´å°‘çš„æ›´æ–°è½®æ¬¡
        for epoch in range(self.n_updates_per_iteration):
            try:
                log_probs, values, entropy = self.policy_network.evaluate_actions(obs_tensor, acts_tensor)

                ratio = torch.exp(log_probs - old_log_probs)
                surr1 = ratio * advantages_tensor
                surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages_tensor
                actor_loss = -torch.min(surr1, surr2).mean()

                critic_loss = torch.nn.MSELoss()(values, returns_tensor)
                total_loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy.mean()

                self.optimizer.zero_grad()
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.policy_network.parameters(), self.max_grad_norm)
                self.optimizer.step()

            except Exception as e:
                print(f"âš ï¸ æ›´æ–°å¤±è´¥: {e}")
                break

    def check_early_stopping(self, current_success_rate):
        """æ—©åœæ£€æŸ¥"""
        self.success_history.append(current_success_rate)

        # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹å–„
        if current_success_rate > self.best_success_rate:
            self.best_success_rate = current_success_rate
            self.patience_counter = 0
            return False, f"ğŸ“ˆ æ–°æœ€ä½³æˆåŠŸç‡: {current_success_rate:.1%}"
        else:
            self.patience_counter += 1

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ—©åœæ¡ä»¶
            if self.patience_counter >= self.patience_limit:
                return True, f"â¹ï¸ æ—©åœè§¦å‘: {self.patience_limit}æ¬¡è¿­ä»£æ— æ”¹å–„"

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡æˆåŠŸç‡
            if current_success_rate >= 0.60:
                return True, f"ğŸ¯ è¾¾åˆ°ç›®æ ‡æˆåŠŸç‡: {current_success_rate:.1%}"

            return False, f"â³ ç­‰å¾…æ”¹å–„: {self.patience_counter}/{self.patience_limit}"

    def save_model(self, path):
        torch.save({
            'policy_network': self.policy_network.state_dict(),
            'pheromone_extractor': self.env.get_pheromone_extractor().state_dict(),
            'success_history': self.success_history,
            'best_success_rate': self.best_success_rate
        }, path)


def train_lightweight_cnn():
    """è½»é‡ç‰ˆCNNè®­ç»ƒä¸»ç¨‹åº"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ å¯åŠ¨è½»é‡ç‰ˆCNNè®­ç»ƒ - è®¾å¤‡: {device}")
    print(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")

    # ä½¿ç”¨8ç»´CNNç‰¹å¾ï¼ˆä¸å¿«é€Ÿæµ‹è¯•ä¸€è‡´ï¼‰
    env = CNNPheromoneEnv(cnn_feature_dim=8)
    aco = ACOSystemMatrix()
    agent = LightweightCNNAgent(env, device=device)

    # è®¡ç®—ç½‘ç»œå‚æ•°
    total_params = sum(p.numel() for p in agent.policy_network.parameters() if p.requires_grad)
    cnn_params = sum(p.numel() for p in env.get_pheromone_extractor().parameters() if p.requires_grad)

    print(f"ğŸ“Š è½»é‡ç‰ˆé…ç½®:")
    print(f"   è§‚æµ‹ç»´åº¦: {env.observation_space.shape[0]} (4åŸºç¡€ + 8CNNç‰¹å¾)")
    print(f"   ç½‘ç»œæ€»å‚æ•°: {total_params:,} (è½»é‡åŒ–)")
    print(f"   CNNå‚æ•°: {cnn_params:,}")
    print(f"   LSTMéšè—å±‚: 64 (vs åŸæ¥256)")
    print(f"   å­¦ä¹ ç‡: 5e-5 (ä¿å®ˆ)")
    print(f"   æ—©åœè€å¿ƒ: {agent.patience_limit} è¿­ä»£")
    print(f"   ç›®æ ‡æˆåŠŸç‡: 60%")

    start_time = time.time()
    iteration = 0
    max_iterations = 100  # æœ€å¤§è¿­ä»£æ¬¡æ•°é™åˆ¶

    print(f"\nğŸ¯ å¼€å§‹è½»é‡ç‰ˆè®­ç»ƒ...")
    print(f"=" * 60)

    while iteration < max_iterations:
        iteration += 1

        try:
            # æ‰§è¡Œrollout
            rollout_data = agent.rollout(aco, episodes_per_batch=30)
            advantages, returns = agent.compute_advantages(rollout_data)

            if len(advantages) > 0:
                agent.update_networks(rollout_data, advantages, returns)

            # è·å–ç»Ÿè®¡ä¿¡æ¯
            success_rate = agent.logger['batch_success_rate']
            avg_reward = np.mean(agent.logger['batch_rews']) if agent.logger['batch_rews'] else 0.0

            # æ—©åœæ£€æŸ¥
            should_stop, stop_message = agent.check_early_stopping(success_rate)

            # è¾“å‡ºè¿›åº¦
            print(f"è¿­ä»£ {iteration:3d} | æˆåŠŸç‡={success_rate:5.1%} | "
                  f"å¥–åŠ±={avg_reward:6.2f} | ç”Ÿæ¶¯æˆåŠŸ={agent._lifetime_successes:3d} | {stop_message}")

            # æ—©åœæ£€æŸ¥
            if should_stop:
                print(f"\n{stop_message}")
                break

            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if iteration % 20 == 0:
                checkpoint_path = f"models/lightweight_cnn_checkpoint_{iteration}.pth"
                agent.save_model(checkpoint_path)
                print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")

        except Exception as e:
            print(f"âŒ è¿­ä»£ {iteration} å¤±è´¥: {e}")
            continue

    # è®­ç»ƒå®Œæˆ
    total_time = time.time() - start_time
    final_success = agent.best_success_rate

    print(f"\n" + "=" * 60)
    print(f"ğŸ è½»é‡ç‰ˆè®­ç»ƒå®Œæˆ!")
    print(f"   æ€»è¿­ä»£: {iteration}")
    print(f"   ç”¨æ—¶: {total_time / 60:.1f} åˆ†é’Ÿ")
    print(f"   æœ€ä½³æˆåŠŸç‡: {final_success:.1%}")
    print(f"   æ€»æˆåŠŸæ¬¡æ•°: {agent._lifetime_successes}")
    print(f"   ç»“æŸæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = "models/lightweight_cnn_final.pth"
    agent.save_model(final_path)
    print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹: {final_path}")

    # ç»˜åˆ¶å­¦ä¹ æ›²çº¿
    if len(agent.success_history) > 0:
        plt.figure(figsize=(10, 6))
        plt.plot(agent.success_history, 'b-', linewidth=2, label='æˆåŠŸç‡')
        plt.axhline(y=0.60, color='r', linestyle='--', label='ç›®æ ‡æˆåŠŸç‡ (60%)')
        plt.axhline(y=agent.best_success_rate, color='g', linestyle='--',
                    label=f'æœ€ä½³æˆåŠŸç‡ ({agent.best_success_rate:.1%})')
        plt.xlabel('è¿­ä»£æ¬¡æ•°')
        plt.ylabel('æˆåŠŸç‡')
        plt.title(f'è½»é‡ç‰ˆCNNè®­ç»ƒè¿›å±• (æœ€ä½³: {agent.best_success_rate:.1%})')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig('lightweight_cnn_learning_curve.png', dpi=150, bbox_inches='tight')
        print(f"ğŸ“ˆ å­¦ä¹ æ›²çº¿ä¿å­˜: lightweight_cnn_learning_curve.png")

    return agent.success_history


if __name__ == "__main__":
    os.makedirs('models', exist_ok=True)

    try:
        success_history = train_lightweight_cnn()

        if len(success_history) > 0 and max(success_history) >= 0.60:
            print(f"\nğŸ‰ è½»é‡ç‰ˆCNNè®­ç»ƒæˆåŠŸ!")
            print(f"   è¯æ˜CNNæ–¹æ³•ç¡®å®æœ‰æ•ˆ")
            print(f"   é—®é¢˜åœ¨äºä¹‹å‰çš„è®­ç»ƒç­–ç•¥è¿‡äºå¤æ‚")
        elif len(success_history) > 0 and max(success_history) >= 0.30:
            print(f"\nâš ï¸ è½»é‡ç‰ˆCNNéƒ¨åˆ†æˆåŠŸ")
            print(f"   æˆåŠŸç‡æœ‰æå‡ä½†æœªè¾¾åˆ°é¢„æœŸ")
            print(f"   éœ€è¦è¿›ä¸€æ­¥è°ƒä¼˜")
        else:
            print(f"\nâŒ è½»é‡ç‰ˆCNNä»ç„¶å¤±è´¥")
            print(f"   é—®é¢˜å¯èƒ½åœ¨äºç®—æ³•è®¾è®¡æœ¬èº«")

    except Exception as e:
        print(f"âŒ è½»é‡ç‰ˆè®­ç»ƒå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()