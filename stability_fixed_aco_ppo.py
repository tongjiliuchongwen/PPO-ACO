# stability_fixed_aco_ppo.py - ä¿®å¤è®­ç»ƒç¨³å®šæ€§é—®é¢˜çš„ACO-PPO
# è§£å†³åæœŸæ€§èƒ½å´©æºƒé—®é¢˜

import numpy as np
import torch
import torch.optim as optim
import matplotlib.pyplot as plt
import time
from copy import deepcopy

import config_matrix as config
from integrated_aco_ppo_training import IntegratedACOPPOEnv, IntegratedACOPPOAgent


class StabilizedACOPPOAgent(IntegratedACOPPOAgent):
    """ç¨³å®šåŒ–çš„ACO-PPOæ™ºèƒ½ä½“ - é˜²æ­¢è®­ç»ƒå´©æºƒ"""

    def __init__(self, env, device='cpu'):
        super().__init__(env, device, learning_rate=3e-4)  # é™ä½åˆå§‹å­¦ä¹ ç‡

        # ğŸ”§ ç¨³å®šåŒ–è¶…å‚æ•°
        self.gamma = 0.99  # æ ‡å‡†æŠ˜æ‰£å› å­
        self.gae_lambda = 0.95  # æ ‡å‡†GAEå‚æ•°
        self.clip_ratio = 0.1  # ä¿å®ˆçš„è£å‰ªæ¯”ä¾‹
        self.entropy_coef = 0.02  # ä¿æŒæ¢ç´¢
        self.value_coef = 0.5  # æ ‡å‡†ä»·å€¼æŸå¤±æƒé‡
        self.max_grad_norm = 0.2  # ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
        self.n_updates_per_iteration = 3  # å‡å°‘æ›´æ–°æ¬¡æ•°

        # ğŸ”§ é€‚åº”æ€§å­¦ä¹ ç‡è°ƒåº¦ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='max', factor=0.8, patience=15
        )

        # ğŸ”§ æ€§èƒ½è·Ÿè¸ªå’Œä¿æŠ¤æœºåˆ¶
        self.best_model_state = None
        self.best_success_rate = 0.0
        self.performance_buffer = []
        self.buffer_size = 10
        self.performance_threshold = 0.05  # æ€§èƒ½ä¸‹é™é˜ˆå€¼
        self.lr_reduced_count = 0  # å­¦ä¹ ç‡é™ä½è®¡æ•°

        print(f"ğŸ›¡ï¸ ç¨³å®šåŒ–æ™ºèƒ½ä½“å‚æ•°:")
        print(f"   ä¿å®ˆclip_ratio={self.clip_ratio}")
        print(f"   ä¸¥æ ¼grad_norm={self.max_grad_norm}")
        print(f"   å‡å°‘æ›´æ–°æ¬¡æ•°={self.n_updates_per_iteration}")

    def train(self, num_iterations=100, episodes_per_iteration=25):
        """å¸¦ç¨³å®šæ€§ä¿æŠ¤çš„è®­ç»ƒå¾ªç¯"""
        print(f"ğŸ›¡ï¸ å¼€å§‹ç¨³å®šåŒ–ACO-PPOè®­ç»ƒ")
        print(f"   è¿­ä»£æ¬¡æ•°: {num_iterations}, æ¯æ¬¡è¿­ä»£episodes: {episodes_per_iteration}")
        print(f"   å½“å‰æ—¶é—´: 2025-08-22 04:29:28")
        print(f"   ç”¨æˆ·: tongjiliuchongwen")

        training_history = {
            'success_rates': [],
            'avg_rewards': [],
            'avg_episode_lengths': [],
            'learning_rates': [],
            'rollbacks': []
        }

        rollback_count = 0
        consecutive_poor_performance = 0

        for iteration in range(num_iterations):
            # æ”¶é›†æ•°æ®
            batch_data = self._collect_batch_data(episodes_per_iteration)
            success_rate = batch_data['success_rate']
            avg_reward = batch_data['avg_reward']

            # ğŸ”§ æ€§èƒ½ç›‘æ§
            self.performance_buffer.append(success_rate)
            if len(self.performance_buffer) > self.buffer_size:
                self.performance_buffer.pop(0)

            # ğŸ”§ ä¿å­˜å½“å‰æœ€ä½³æ¨¡å‹
            if success_rate > self.best_success_rate:
                self.best_success_rate = success_rate
                self.best_model_state = deepcopy(self.policy_network.state_dict())
                consecutive_poor_performance = 0
                print(f"   ğŸ’ æ–°æœ€ä½³æ¨¡å‹: {success_rate:.1%}")

            # ğŸ”§ æ£€æµ‹æ€§èƒ½å´©æºƒ
            performance_drop = False
            if len(self.performance_buffer) >= self.buffer_size:
                recent_avg = np.mean(self.performance_buffer[-5:])
                earlier_avg = np.mean(self.performance_buffer[:5])

                if earlier_avg > 0.1 and recent_avg < earlier_avg - self.performance_threshold:
                    performance_drop = True
                    consecutive_poor_performance += 1

            # ğŸ”§ æ€§èƒ½å›æ»šæœºåˆ¶
            if performance_drop and consecutive_poor_performance >= 3 and self.best_model_state is not None:
                print(f"   ğŸ”„ æ£€æµ‹åˆ°æ€§èƒ½å´©æºƒï¼å›æ»šåˆ°æœ€ä½³æ¨¡å‹...")
                self.policy_network.load_state_dict(self.best_model_state)
                rollback_count += 1
                consecutive_poor_performance = 0

                # é™ä½å­¦ä¹ ç‡
                for param_group in self.optimizer.param_groups:
                    param_group['lr'] *= 0.5
                self.lr_reduced_count += 1
                print(f"   ğŸ“‰ å­¦ä¹ ç‡é™ä½åˆ°: {self.optimizer.param_groups[0]['lr']:.2e}")

            # ğŸ”§ è°¨æ…çš„ç½‘ç»œæ›´æ–°
            if batch_data['observations'] and not performance_drop:
                self._stable_update_network(batch_data)

            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(success_rate)
            current_lr = self.optimizer.param_groups[0]['lr']

            # è®°å½•ç»Ÿè®¡
            training_history['success_rates'].append(success_rate)
            training_history['avg_rewards'].append(avg_reward)
            training_history['avg_episode_lengths'].append(batch_data['avg_episode_length'])
            training_history['learning_rates'].append(current_lr)
            training_history['rollbacks'].append(rollback_count)

            # è¾“å‡ºè¿›åº¦
            if iteration % 5 == 0 or iteration < 15:
                status_icon = "ğŸ”„" if performance_drop else "âœ…"
                print(f"{status_icon} è¿­ä»£ {iteration + 1:3d} | æˆåŠŸç‡={success_rate:5.1%} | "
                      f"å¥–åŠ±={avg_reward:7.2f} | æ€»æˆåŠŸ={self._lifetime_successes} | "
                      f"LR={current_lr:.2e} | å›æ»š={rollback_count}")

            # é‡Œç¨‹ç¢‘æŠ¥å‘Š
            if (iteration + 1) % 20 == 0:
                recent_success_rate = np.mean(training_history['success_rates'][-10:])
                print(f"ğŸ“Š è¿­ä»£ {iteration + 1} é‡Œç¨‹ç¢‘:")
                print(f"   æœ€è¿‘10æ¬¡å¹³å‡: {recent_success_rate:.1%}")
                print(f"   å†å²æœ€ä½³: {self.best_success_rate:.1%}")
                print(f"   æ€»å›æ»šæ¬¡æ•°: {rollback_count}")
                print(f"   å­¦ä¹ ç‡è°ƒæ•´æ¬¡æ•°: {self.lr_reduced_count}")

        print(f"ğŸ ç¨³å®šåŒ–è®­ç»ƒå®Œæˆ!")
        print(f"   æœ€ä½³æˆåŠŸç‡: {self.best_success_rate:.1%}")
        print(f"   æ€»å›æ»šæ¬¡æ•°: {rollback_count}")
        print(f"   å­¦ä¹ ç‡è°ƒæ•´æ¬¡æ•°: {self.lr_reduced_count}")

        # æœ€ç»ˆåŠ è½½æœ€ä½³æ¨¡å‹
        if self.best_model_state is not None:
            print(f"   ğŸ¯ åŠ è½½æœ€ä½³æ¨¡å‹ç”¨äºæµ‹è¯•")
            self.policy_network.load_state_dict(self.best_model_state)

        return training_history

    def _stable_update_network(self, batch_data):
        """ç¨³å®šçš„ç½‘ç»œæ›´æ–°ï¼Œå¢åŠ é¢å¤–ä¿æŠ¤"""
        try:
            # æ ‡å‡†PPOæ›´æ–°æµç¨‹ï¼ˆå‡å°‘æ›´æ–°æ¬¡æ•°ï¼‰
            batch_obs = np.array(batch_data['observations'], dtype=np.float32)
            batch_acts = np.array(batch_data['actions'], dtype=np.float32)
            batch_logp = np.array(batch_data['log_probs'], dtype=np.float32)

            # è®¡ç®—ä»·å€¼å’Œä¼˜åŠ¿
            with torch.no_grad():
                obs_tensor = torch.from_numpy(batch_obs).to(self.device)
                _, _, values, _ = self.policy_network.forward(obs_tensor, None)
                values = values.cpu().numpy()

            # GAEè®¡ç®—
            all_advantages = []
            all_returns = []

            start_idx = 0
            for ep_rewards in batch_data['rewards_list']:
                ep_len = len(ep_rewards)
                ep_values = values[start_idx:start_idx + ep_len]

                advantages = np.zeros_like(ep_rewards, dtype=np.float32)
                last_advantage = 0.0

                for t in reversed(range(ep_len)):
                    next_value = ep_values[t + 1] if t < ep_len - 1 else 0.0
                    delta = ep_rewards[t] + self.gamma * next_value - ep_values[t]
                    advantages[t] = delta + self.gamma * self.gae_lambda * last_advantage
                    last_advantage = advantages[t]

                returns = advantages + ep_values
                all_advantages.extend(advantages)
                all_returns.extend(returns)
                start_idx += ep_len

            if len(all_advantages) == 0:
                return

            # ğŸ”§ ç¨³å®šçš„ä¼˜åŠ¿å½’ä¸€åŒ–
            all_advantages = np.array(all_advantages, dtype=np.float32)
            if all_advantages.std() > 1e-6:
                all_advantages = (all_advantages - all_advantages.mean()) / (all_advantages.std() + 1e-8)
                # é™åˆ¶ä¼˜åŠ¿å€¼èŒƒå›´
                all_advantages = np.clip(all_advantages, -5.0, 5.0)

            # è½¬æ¢åˆ°GPU
            obs_tensor = torch.from_numpy(batch_obs).float().to(self.device)
            acts_tensor = torch.from_numpy(batch_acts).float().to(self.device)
            old_log_probs = torch.from_numpy(batch_logp).float().to(self.device)
            advantages_tensor = torch.from_numpy(all_advantages).float().to(self.device)
            returns_tensor = torch.from_numpy(np.array(all_returns, dtype=np.float32)).float().to(self.device)

            # ğŸ”§ ç¨³å®šçš„PPOæ›´æ–°ï¼ˆå‡å°‘æ›´æ–°æ¬¡æ•°ï¼‰
            for epoch in range(self.n_updates_per_iteration):
                log_probs, values, entropy = self.policy_network.evaluate_actions(obs_tensor, acts_tensor)

                ratio = torch.exp(log_probs - old_log_probs)
                surr1 = ratio * advantages_tensor
                surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages_tensor
                actor_loss = -torch.min(surr1, surr2).mean()

                critic_loss = torch.nn.MSELoss()(values, returns_tensor)
                total_loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy.mean()

                # ğŸ”§ æ£€æŸ¥æŸå¤±æ˜¯å¦æ­£å¸¸
                if torch.isnan(total_loss) or torch.isinf(total_loss):
                    print(f"   âš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸æŸå¤±ï¼Œè·³è¿‡æ­¤æ¬¡æ›´æ–°")
                    break

                self.optimizer.zero_grad()
                total_loss.backward()

                # ğŸ”§ ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
                grad_norm = torch.nn.utils.clip_grad_norm_(self.policy_network.parameters(), self.max_grad_norm)

                # ğŸ”§ æ¢¯åº¦å¼‚å¸¸æ£€æŸ¥
                if grad_norm > 5.0:
                    print(f"   âš ï¸ æ¢¯åº¦è¿‡å¤§ ({grad_norm:.2f})ï¼Œè·³è¿‡æ­¤æ¬¡æ›´æ–°")
                    continue

                self.optimizer.step()

        except Exception as e:
            print(f"âš ï¸ ç½‘ç»œæ›´æ–°å¤±è´¥: {e}")


class StabilizedACOPPOEnv(IntegratedACOPPOEnv):
    """ç¨³å®šåŒ–ç¯å¢ƒ - å‡å°‘å¥–åŠ±æ³¢åŠ¨"""

    def __init__(self):
        super().__init__(use_cnn_features=True)

        # ğŸ”§ æ›´ç¨³å®šçš„å¥–åŠ±å‚æ•°
        self.step_penalty = -0.01  # æ ‡å‡†æ­¥éª¤æƒ©ç½š
        self.target_reward = 50.0  # é€‚ä¸­çš„æˆåŠŸå¥–åŠ±
        self.collision_penalty = -15.0  # é€‚ä¸­çš„ç¢°æ’æƒ©ç½š

        # æ›´ä¿å®ˆçš„ä¿¡æ¯ç´ å¥–åŠ±å‚æ•°
        self.pheromone_alpha = 2.0
        self.pheromone_beta = 4.0
        self.pheromone_threshold = 0.08

        # å‡å°‘è·ç¦»å¡‘å½¢å¥–åŠ±çš„å½±å“
        self.distance_shaping_coef = 0.5
        self.prev_distance = None

        print(f"ğŸ›¡ï¸ ç¨³å®šåŒ–ç¯å¢ƒå‚æ•°:")
        print(f"   ä¿å®ˆå¥–åŠ±è®¾è®¡ï¼Œå‡å°‘æ³¢åŠ¨")
        print(f"   ä¿¡æ¯ç´ å¥–åŠ±ç³»æ•°: Î±={self.pheromone_alpha}, Î²={self.pheromone_beta}")

    def reset(self, seed=None, options=None, major_reset=False):
        obs, info = super().reset(seed, options, major_reset)
        self.prev_distance = np.linalg.norm(self.agent_pos - self.target_pos)
        return obs, info

    def _calculate_aco_reward(self):
        """ç¨³å®šåŒ–çš„å¥–åŠ±è®¡ç®—"""
        # åŸºç¡€ACOå¥–åŠ±
        reward = self.step_penalty

        # è·å–ä¿¡æ¯ç´ ä¿¡æ¯
        nav_concentration = self.aco_system.get_average_nav_pheromone(self.agent_pos)
        nav_gradient = self.aco_system.get_nav_gradient(self.agent_pos)
        gradient_mag = np.linalg.norm(nav_gradient)

        # æµ“åº¦å¥–åŠ±
        if nav_concentration > self.pheromone_threshold:
            concentration_reward = self.pheromone_alpha * (nav_concentration - self.pheromone_threshold)
            reward += concentration_reward

        # æ¢¯åº¦å¥–åŠ±
        if gradient_mag > 0.001:
            gradient_reward = self.pheromone_beta * gradient_mag
            reward += gradient_reward

        # ğŸ”§ æ·»åŠ é€‚åº¦çš„è·ç¦»å¡‘å½¢å¥–åŠ±
        current_distance = np.linalg.norm(self.agent_pos - self.target_pos)
        if self.prev_distance is not None:
            distance_improvement = self.prev_distance - current_distance
            distance_reward = self.distance_shaping_coef * distance_improvement
            reward += distance_reward

        self.prev_distance = current_distance

        # ğŸ”§ æ¥è¿‘ç›®æ ‡çš„é€‚åº¦å¥–åŠ±
        if current_distance < 2.0:
            proximity_reward = 3.0 * (2.0 - current_distance) / 2.0
            reward += proximity_reward

        return reward


def stability_test():
    """ç¨³å®šæ€§æµ‹è¯•"""
    print(f"ğŸ›¡ï¸ ACO-PPOç¨³å®šæ€§æµ‹è¯•")
    print(f"   å½“å‰æ—¶é—´: 2025-08-22 04:29:28")
    print(f"   ç”¨æˆ·: tongjiliuchongwen")
    print(f"=" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")

    # ä½¿ç”¨ç¨³å®šåŒ–ç‰ˆæœ¬
    env = StabilizedACOPPOEnv()
    agent = StabilizedACOPPOAgent(env, device)

    # ç¨³å®šåŒ–è®­ç»ƒ
    print(f"ğŸ›¡ï¸ è¿›è¡Œç¨³å®šåŒ–è®­ç»ƒ (60æ¬¡è¿­ä»£)...")
    history = agent.train(num_iterations=60, episodes_per_iteration=25)

    # æµ‹è¯•ç¨³å®šæ€§
    print(f"\nğŸ§ª ç¨³å®šæ€§æµ‹è¯•...")
    test_results = []
    for i in range(8):
        success_rate = test_comprehensive(env, agent, 20)
        test_results.append(success_rate)
        print(f"   æµ‹è¯•è½®æ¬¡{i + 1}: {success_rate:.1%}")

    avg_test_success = np.mean(test_results)
    test_std = np.std(test_results)
    stability_score = 1 - test_std / max(avg_test_success, 0.01)

    print(f"\nğŸ“Š ç¨³å®šæ€§æµ‹è¯•ç»“æœ:")
    print(f"   å¹³å‡æµ‹è¯•æˆåŠŸç‡: {avg_test_success:.1%} Â± {test_std:.1%}")
    print(f"   æµ‹è¯•ç¨³å®šæ€§è¯„åˆ†: {stability_score:.1%}")
    print(f"   æœ€ç»ˆè®­ç»ƒæˆåŠŸç‡: {history['success_rates'][-1]:.1%}")
    print(f"   æœ€ä½³å†å²æˆåŠŸç‡: {agent.best_success_rate:.1%}")
    print(f"   è®­ç»ƒå›æ»šæ¬¡æ•°: {history['rollbacks'][-1]}")

    # åˆ†æè®­ç»ƒç¨³å®šæ€§
    success_rates = history['success_rates']
    if len(success_rates) > 30:
        early_phase = np.mean(success_rates[10:20])
        mid_phase = np.mean(success_rates[25:35])
        late_phase = np.mean(success_rates[-10:])

        print(f"\nğŸ“ˆ è®­ç»ƒé˜¶æ®µåˆ†æ:")
        print(f"   æ—©æœŸé˜¶æ®µ (10-20): {early_phase:.1%}")
        print(f"   ä¸­æœŸé˜¶æ®µ (25-35): {mid_phase:.1%}")
        print(f"   åæœŸé˜¶æ®µ (æœ€å10æ¬¡): {late_phase:.1%}")

        # åˆ¤æ–­ç¨³å®šæ€§
        if late_phase >= mid_phase * 0.8:
            print(f"   âœ… è®­ç»ƒç¨³å®šï¼Œæ— æ˜æ˜¾æ€§èƒ½å´©æºƒ")
            stability_ok = True
        else:
            print(f"   âš ï¸ è®­ç»ƒåæœŸæœ‰æ€§èƒ½ä¸‹é™")
            stability_ok = False
    else:
        stability_ok = True

    # å¯è§†åŒ–ç¨³å®šæ€§
    plt.figure(figsize=(15, 8))

    # æˆåŠŸç‡æ›²çº¿
    plt.subplot(2, 3, 1)
    plt.plot(history['success_rates'], 'b-', linewidth=2, label='Success Rate')
    if agent.best_success_rate > 0:
        plt.axhline(y=agent.best_success_rate, color='r', linestyle='--', alpha=0.7, label='Best Rate')
    plt.title('Training Success Rate (Stabilized)')
    plt.xlabel('Iteration')
    plt.ylabel('Success Rate')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # å›æ»šæ¬¡æ•°
    plt.subplot(2, 3, 2)
    plt.plot(history['rollbacks'], 'orange', linewidth=2)
    plt.title('Model Rollbacks')
    plt.xlabel('Iteration')
    plt.ylabel('Rollback Count')
    plt.grid(True, alpha=0.3)

    # å­¦ä¹ ç‡å˜åŒ–
    plt.subplot(2, 3, 3)
    plt.plot(history['learning_rates'], 'green', linewidth=2)
    plt.title('Learning Rate Schedule')
    plt.xlabel('Iteration')
    plt.ylabel('Learning Rate')
    plt.yscale('log')
    plt.grid(True, alpha=0.3)

    # æµ‹è¯•ç»“æœ
    plt.subplot(2, 3, 4)
    plt.bar(range(1, len(test_results) + 1), test_results, alpha=0.7)
    plt.axhline(y=avg_test_success, color='r', linestyle='--', label=f'Avg: {avg_test_success:.1%}')
    plt.title('Test Stability')
    plt.xlabel('Test Round')
    plt.ylabel('Success Rate')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # å¥–åŠ±ç¨³å®šæ€§
    plt.subplot(2, 3, 5)
    plt.plot(history['avg_rewards'], 'purple', linewidth=2)
    plt.title('Reward Stability')
    plt.xlabel('Iteration')
    plt.ylabel('Average Reward')
    plt.grid(True, alpha=0.3)

    # æ€§èƒ½åˆ†å¸ƒ
    plt.subplot(2, 3, 6)
    if len(set(history['success_rates'])) > 1:  # æ£€æŸ¥æ˜¯å¦æœ‰å˜åŒ–
        plt.hist(history['success_rates'], bins=max(5, len(set(history['success_rates']))), alpha=0.7,
                 edgecolor='black')
    else:
        plt.bar([0], [len(history['success_rates'])], alpha=0.7)
    plt.axvline(x=np.mean(history['success_rates']), color='r', linestyle='--',
                label=f'Mean: {np.mean(history["success_rates"]):.1%}')
    plt.title('Success Rate Distribution')
    plt.xlabel('Success Rate')
    plt.ylabel('Frequency')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('stability_test_results.png', dpi=150, bbox_inches='tight')
    plt.close()

    print(f"ğŸ“Š ä¿å­˜ç¨³å®šæ€§æµ‹è¯•å›¾: stability_test_results.png")

    return avg_test_success, stability_score


def test_comprehensive(env, agent, num_episodes):
    """æµ‹è¯•å‡½æ•°"""
    successes = 0
    for episode in range(num_episodes):
        obs, _ = env.reset(major_reset=True)
        hidden_state = None
        done = truncated = False
        steps = 0

        while not (done or truncated) and steps < 200:
            action, _, hidden_state = agent.get_action(obs, hidden_state)
            obs, reward, done, truncated, info = env.step(action)
            steps += 1

        if info.get('target_found', False):
            successes += 1

    return successes / num_episodes


if __name__ == "__main__":
    start_time = time.time()

    avg_success, stability = stability_test()

    total_time = time.time() - start_time

    print(f"\nğŸ ç¨³å®šæ€§æµ‹è¯•æ€»ç»“:")
    print(f"   ç”¨æ—¶: {total_time / 60:.1f} åˆ†é’Ÿ")
    print(f"   å¹³å‡æˆåŠŸç‡: {avg_success:.1%}")
    print(f"   ç¨³å®šæ€§è¯„åˆ†: {stability:.1%}")

    if stability > 0.8 and avg_success > 0.15:
        print(f"ğŸ‰ è®­ç»ƒç¨³å®šä¸”æœ‰æ•ˆï¼")
    elif stability > 0.8:
        print(f"âœ… è®­ç»ƒç¨³å®šï¼Œä½†æˆåŠŸç‡éœ€æå‡")
    elif avg_success > 0.15:
        print(f"âœ… æˆåŠŸç‡è‰¯å¥½ï¼Œä½†ç¨³å®šæ€§éœ€æ”¹å–„")
    else:
        print(f"âš ï¸ ä»æœ‰ç¨³å®šæ€§å’Œæ€§èƒ½é—®é¢˜éœ€è§£å†³")

    print(f"\nğŸ’¡ å»ºè®®:")
    if avg_success < 0.1:
        print(f"   - æˆåŠŸç‡å¤ªä½ï¼Œæ£€æŸ¥å¥–åŠ±å‡½æ•°è®¾è®¡")
        print(f"   - è€ƒè™‘å¢åŠ ä¿¡æ¯ç´ å¼•å¯¼å¼ºåº¦")
    elif stability < 0.7:
        print(f"   - ç¨³å®šæ€§ä¸è¶³ï¼Œéœ€è¦æ›´ä¿å®ˆçš„è®­ç»ƒç­–ç•¥")
        print(f"   - è€ƒè™‘è¿›ä¸€æ­¥å‡å°å­¦ä¹ ç‡å’Œæ¢¯åº¦è£å‰ª")
    else:
        print(f"   - è¡¨ç°è‰¯å¥½ï¼Œå¯è€ƒè™‘å¢åŠ è®­ç»ƒæ—¶é—´æˆ–ä¼˜åŒ–ç½‘ç»œæ¶æ„")